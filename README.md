# NiFi REST API Monitor

A modular Python application to collect, monitor, and store a wide range of Apache NiFi metrics via the REST API. It is designed for clustered environments and provides detailed insights into the health and performance of your data flows.

### Key Features:
- **Designed for NiFi Clusters**: Uses a "Distributed Agent" model with a templated configuration for easy deployment.
- **Flexible Storage Options**: Write metrics to **AWS S3**, **Azure Blob Storage**, or the **Local File System**.
- **Dynamic & Granular Scheduling**: Set unique collection intervals for each metric type and update them on the fly.
- **Targeted Monitoring**: Monitor all components, specific flows (Process Groups), or even individual processors by name.
- **Interactive Troubleshooting CLI**: An analysis tool to load and inspect collected metrics for point-in-time diagnostics.
- **Comprehensive Unit Tests**: Includes a `pytest` suite to ensure reliability and prevent regressions.

---

## ğŸš€ Deployment Model: Distributed Agent

This monitor is designed to run as a **distributed agent**, which is the recommended pattern for monitoring a NiFi cluster. An instance of the script is deployed on **every node** in the cluster.

---

## ğŸš€ Install the python project: 

pip install -e .

---

## ğŸ“¦ Project Structure
```
nifi_metrics_collector/
â”œâ”€â”€ bin/
â”‚   â””â”€â”€ run_collector.py       # Main entrypoint & scheduler
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ ... (Core logic modules)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ nifi_config.json       # Templated NiFi connection and metric settings
â”‚   â””â”€â”€ secrets.json           # Credentials & storage settings
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ ... (Test files)
â”œâ”€â”€ pyproject.toml             # Project definition
â””â”€â”€ README.md
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ lib
â”‚       â””â”€â”€ ... (Core logic modules)
â”‚   â””â”€â”€ troubleshoot.py        # Analysis script for adhoc investigation
```

---
## âš™ï¸ Configuration

#### 1. `config/nifi-config.json`
This file acts as a **template**. You can use the same file across all nodes. The `{hostname}` placeholder will be dynamically replaced by the `--hostname` argument passed to the `run_collector.py` script at runtime.

```json
{
  "nifi_api_url": "https://{hostname}:8443/nifi-api",
  "nifi_token_url": "https://{hostname}:8443/nifi-api/access/token",
  "...": "..."
}
```

#### 2. `config/secrets.json`
This file should be the **same across all nodes** to ensure they all write to the same central storage location.

---

## ğŸš€ Part 1: Metric Collection

The following steps should be performed on each node in the NiFi cluster.

#### 1. Install Collector
```bash
# Create and activate a virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install the project in editable mode
pip install -e .
```

#### 2. Run the Collector
Use the `--hostname` argument to specify which NiFi instance to monitor.

```bash
# For the distributed agent model, run on each node pointing to itself (default)
python bin/run_collector.py

# Explicitly pointing to localhost
python bin/run_collector.py --hostname localhost

# To monitor a remote node from a central server
python bin/run_collector.py --hostname nifi-node-01.your.domain.com

# Run once for debugging
python bin/run_collector.py --hostname localhost --once
```

---

## ğŸ” Part 2: Interactive Troubleshooting Tool

The `troubleshoot.py` script provides an interactive CLI to load and analyze the JSON metrics generated by the collector.

#### 1. Install Analysis Dependencies
```bash
# From your activated virtual environment
pip install -r analysis/requirements-analysis.txt
```

#### 2. Run the Analysis Tool
```bash
python analysis/troubleshoot.py
```

---

## âœ… Testing
...

